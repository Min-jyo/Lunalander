# libraries
import gymnasium as gym
import collections
import random
import numpy as np

# pytorch library is used for deep learning
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# ğŸ’¡ íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë¦¬ë¥¼ ìœ„í•´ os ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import os 
# ğŸ’¡ ê·¸ë˜í”„ ì¶œë ¥ì„ ìœ„í•´ matplotlib ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import matplotlib.pyplot as plt

# hyperparameters
learning_rate = 0.005
gamma = 0.98
buffer_limit = 50000        # size of replay buffer
batch_size = 32
tau = 1e-3                  # for soft update
epsilon = 1.0               # Epsilonì„ ì „ì—­ ë³€ìˆ˜ë¡œ ê´€ë¦¬
PRINT_INTERVAL = 20         # ğŸ’¡ í‰ê·  ì ìˆ˜ ê³„ì‚° ë° ì¶œë ¥ ê°„ê²© (Globalë¡œ ì •ì˜)

class ReplayBuffer():
    def __init__(self):
        self.buffer = collections.deque(maxlen=buffer_limit)    # double-ended queue

    def put(self, transition):
        self.buffer.append(transition)

    def sample(self, n):
        mini_batch = random.sample(self.buffer, n)
        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []

        for transition in mini_batch:
            s, a, r, s_prime, done_mask = transition
            s_lst.append(s)
            a_lst.append([a])
            r_lst.append([r])
            s_prime_lst.append(s_prime)
            done_mask_lst.append([done_mask])

        return torch.tensor(np.array(s_lst), dtype=torch.float), torch.tensor(a_lst), \
               torch.tensor(r_lst, dtype=torch.float), torch.tensor(np.array(s_prime_lst), dtype=torch.float), \
               torch.tensor(done_mask_lst, dtype=torch.float)

    def size(self):
        return len(self.buffer)

class Qnet(nn.Module):
    def __init__(self):
        super(Qnet, self).__init__()
        self.fc1 = nn.Linear(8, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 4)

    def forward(self, x):
        # ğŸ› ì˜¤ë¥˜ í•´ê²°: ë‹¨ì¼ ìƒ˜í”Œì¼ ê²½ìš° ë°°ì¹˜ ì°¨ì›(dim=0)ì´ ì—†ìœ¼ë¯€ë¡œ,
        # ë°°ì¹˜ ì…ë ¥ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ unsqueeze(0)ë¥¼ ì ìš©í•˜ë„ë¡ ì²˜ë¦¬
        if x.dim() == 1:
            x = x.unsqueeze(0)
            
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def sample_action(self, obs, epsilon):
        # ğŸ› ì˜¤ë¥˜ í•´ê²°: forward í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ unsqueezeë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” obsë§Œ ì „ë‹¬
        out = self.forward(obs)
        coin = random.random()
        if coin < epsilon:
            return random.randint(0,3)
        else:
            # ë‹¨ì¼ ì…ë ¥ ì‹œ outì€ [1, 4] í¬ê¸°ì´ë¯€ë¡œ dim=1ì—ì„œ argmaxë¥¼ ì·¨í•©ë‹ˆë‹¤.
            return out.argmax(dim=1).item() 

class DuelingQnet(nn.Module):
    def __init__(self):
        super(DuelingQnet, self).__init__()
        self.fc1 = nn.Linear(8, 128)
        self.fc_value = nn.Linear(128, 128)
        self.fc_adv = nn.Linear(128, 128)
        self.value = nn.Linear(128, 1)
        self.adv = nn.Linear(128, 4)

    def forward(self, x):
        # ğŸ› ì˜¤ë¥˜ í•´ê²°: ë‹¨ì¼ ìƒ˜í”Œì¼ ê²½ìš° ë°°ì¹˜ ì°¨ì›(dim=0)ì´ ì—†ìœ¼ë¯€ë¡œ,
        # ë°°ì¹˜ ì…ë ¥ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ unsqueeze(0)ë¥¼ ì ìš©í•˜ë„ë¡ ì²˜ë¦¬
        if x.dim() == 1:
            x = x.unsqueeze(0)
            
        x = F.relu(self.fc1(x))
        v = F.relu(self.fc_value(x))
        a = F.relu(self.fc_adv(x))
        v = self.value(v)
        a = self.adv(a)
        
        # ğŸ› ì˜¤ë¥˜ í•´ê²°: ì´ì œ xê°€ ìµœì†Œ [1, 8] í˜•íƒœì´ë¯€ë¡œ aëŠ” [ë°°ì¹˜í¬ê¸°, 4] í˜•íƒœë¥¼ ê°€ì§
        # ë”°ë¼ì„œ dim=1ì—ì„œ í‰ê· ì„ ì·¨í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.
        a_avg = torch.mean(a, dim=1, keepdim=True)
        q = v + a - a_avg
        return q

    def sample_action(self, obs, epsilon):
        # ğŸ› ì˜¤ë¥˜ í•´ê²°: forward í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ unsqueezeë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” obsë§Œ ì „ë‹¬
        out = self.forward(obs)
        coin = random.random()
        if coin < epsilon:
            return random.randint(0,3)
        else:
            # ë‹¨ì¼ ì…ë ¥ ì‹œ outì€ [1, 4] í¬ê¸°ì´ë¯€ë¡œ dim=1ì—ì„œ argmaxë¥¼ ì·¨í•©ë‹ˆë‹¤.
            return out.argmax(dim=1).item()

def train_dqn(q, q_target, memory, optimizer, tau):
    """Standard DQN training"""
    s,a,r,s_prime,done_mask = memory.sample(batch_size)

    q_out = q(s)
    q_a = q_out.gather(1,a)

    # DQN
    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)

    target = r + gamma * max_q_prime * done_mask
    loss = F.mse_loss(q_a, target)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Soft update
    for target_param, local_param in zip(q_target.parameters(), q.parameters()):
        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)

def train_double_dqn(q, q_target, memory, optimizer, tau):
    """Double DQN training"""
    s,a,r,s_prime,done_mask = memory.sample(batch_size)

    q_out = q(s)
    q_a = q_out.gather(1,a)

    # Double DQN
    argmax_Q = q(s_prime).max(1)[1].unsqueeze(1)
    max_q_prime = q_target(s_prime).gather(1, argmax_Q)

    target = r + gamma * max_q_prime * done_mask
    loss = F.mse_loss(q_a, target)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Soft update
    for target_param, local_param in zip(q_target.parameters(), q.parameters()):
        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)

# ğŸ’¡ run_experiment í•¨ìˆ˜ë¥¼ ì›ë˜ì˜ ì¸ì êµ¬ì„±ìœ¼ë¡œ ë˜ëŒë¦¼ (lr, decay_rate ì¸ì ì œê±°)
def run_experiment(algorithm_type="DQN", render=False, load_model=False):
    """ì§€ì •ëœ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì‹¤í—˜ì„ ì‹¤í–‰í•˜ê³  í•™ìŠµëœ ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global epsilon
    
    # ğŸ“Œ ê³ ì •ëœ ì „ì—­ ë³€ìˆ˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©ì„ ëª…ì‹œ
    print(f"\n=== Running {algorithm_type} Experiment (LR={learning_rate}, Decay=0.995) ===")

    if render:
        env = gym.make('LunarLander-v3', render_mode='human')
        print("Rendering enabled - GUI window will show LunarLander visualization")
    else:
        env = gym.make('LunarLander-v3')

    # ë„¤íŠ¸ì›Œí¬ ì„ íƒ
    if algorithm_type == "Dueling_DQN":
        q = DuelingQnet()
        q_target = DuelingQnet()
        train_fn = train_double_dqn
    else:
        q = Qnet()
        q_target = Qnet()
        train_fn = train_dqn if algorithm_type == "DQN" else train_double_dqn

    model_path = f'./{algorithm_type}_q_net.pth'
    
    # ğŸ“Œ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (Load Model)
    if load_model and os.path.exists(model_path):
        print(f"Loading previous model from {model_path} to continue training.")
        try:
            q.load_state_dict(torch.load(model_path))
            q_target.load_state_dict(q.state_dict())
            epsilon = 0.1 
        except Exception as e:
            print(f"Error loading model state: {e}. Starting new training.")
            q_target.load_state_dict(q.state_dict())
            epsilon = 1.0
    else:
        q_target.load_state_dict(q.state_dict())
        epsilon = 1.0 # ìƒˆë¡œ í•™ìŠµ ì‹œì‘ ì‹œ epsilon ì´ˆê¸°í™”

    memory = ReplayBuffer()

    score = 0.0
    # ğŸ’¡ ê³ ì •ëœ ì „ì—­ ë³€ìˆ˜ learning_rate ì‚¬ìš©
    optimizer = optim.Adam(q.parameters(), lr=learning_rate) 
    
    score_history = []
    
    for n_epi in range(1000): 
        s, _ = env.reset()
        done = False

        while not done:
            a = q.sample_action(torch.from_numpy(s).float(), epsilon)
            s_prime, r, terminated, truncated, info = env.step(a)
            done = (terminated or truncated)
            done_mask = 0.0 if done else 1.0
            memory.put((s,a,r,s_prime, done_mask))
            s = s_prime

            score += r

            if memory.size()>2000:
                train_fn(q, q_target, memory, optimizer, tau)

            if done:
                break
        
        # ğŸ’¡ ê³ ì •ëœ decay rate 0.999 ì‚¬ìš©
        epsilon = max(0.01, epsilon * 0.995) 

        if n_epi % PRINT_INTERVAL == 0 and n_epi != 0: 
            avg_score = score / PRINT_INTERVAL
            print("n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%".format(
                                                            n_epi, avg_score, memory.size(), epsilon*100))
            score_history.append((n_epi, avg_score))
            score = 0.0

    env.close()
    
    # ğŸ“Œ ëª¨ë¸ ì €ì¥ (Save Model)
    torch.save(q.state_dict(), model_path)
    print(f"\nModel for {algorithm_type} saved to {model_path}")
    
    return q, q_target, score_history 

def evaluate_model(q_net, env_name='LunarLander-v3', num_episodes=100, render=False):
    """
    ìµœì¢… í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥(í‰ê·  ë¦¬í„´ê°’, ì„±ê³µë¥ , í‰ê·  ê¸¸ì´)ì„ í‰ê°€í•©ë‹ˆë‹¤.
    ì„±ê³µ ê¸°ì¤€ì€ ìŠ¤ì½”ì–´ 200ì  ì´ìƒì…ë‹ˆë‹¤.
    """
    # ğŸ“Œ íŠœë‹ ëª¨ë“œ ê´€ë ¨ ì¶œë ¥ ì œê±° (ë‹¨ì¼ ì‹¤í—˜ ëª¨ë“œ)
    print(f"\n=== Evaluating Model over {num_episodes} episodes ===")
    
    if render:
        env = gym.make(env_name, render_mode='human')
    else:
        env = gym.make(env_name)
    
    total_score = 0.0
    success_count = 0  # ì„±ê³µ íšŸìˆ˜ ì¹´ìš´í„°
    total_length = 0.0 # ì´ ì—í”¼ì†Œë“œ ê¸¸ì´ ëˆ„ì  ë³€ìˆ˜
    
    epsilon_eval = 0.0 # í‰ê°€ ì‹œ íƒí—˜ ì—†ìŒ (ìµœì  í–‰ë™ë§Œ)

    for n_epi in range(num_episodes):
        s, _ = env.reset()
        done = False
        episode_score = 0.0
        episode_length = 0 

        with torch.no_grad(): # í‰ê°€ ì‹œì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¶ˆí•„ìš”
            while not done:
                a = q_net.sample_action(torch.from_numpy(s).float(), epsilon_eval)
                s_prime, r, terminated, truncated, info = env.step(a)
                done = (terminated or truncated)
                s = s_prime
                episode_score += r
                episode_length += 1
                
                if done:
                    break
        
        total_score += episode_score
        
        if episode_score >= 200:
            success_count += 1
            
        total_length += episode_length

    env.close()
    
    avg_return = total_score / num_episodes
    success_rate = (success_count / num_episodes) * 100
    avg_length = total_length / num_episodes

    # ğŸ“Œ íŠœë‹ ëª¨ë“œ ê´€ë ¨ ì¶œë ¥ ì œê±° (ë‹¨ì¼ ì‹¤í—˜ ëª¨ë“œ)
    print(f"\n[Evaluation Results - {num_episodes} Episodes]")
    print(f"âœ… Average Return: {avg_return:.2f}")
    print(f"ğŸ‰ Success Rate (Score >= 200): {success_rate:.2f}%")
    print(f"â±ï¸ Average Episode Length: {avg_length:.1f} steps")
    
    return avg_return, success_rate, avg_length

def plot_results(all_history, algorithms):
    """í•™ìŠµ ê³¡ì„ (í‰ê·  ë¦¬í„´ê°’ vs ì—í”¼ì†Œë“œ)ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    print("\n=== Generating Learning Curve Plot ===")
    
    plt.figure(figsize=(10, 6))
    
    for history, alg_name in zip(all_history, algorithms):
        episodes = [item[0] for item in history]
        scores = [item[1] for item in history]
        
        # ê° ì•Œê³ ë¦¬ì¦˜ë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ ë¼ì¸ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
        plt.plot(episodes, scores, label=alg_name)

    plt.title('DQN Algorithms Performance Comparison (Average Return)')
    plt.xlabel(f'Episode (Average of {PRINT_INTERVAL} episodes)') 
    plt.ylabel('Average Return (Score)')
    plt.grid(True, linestyle='--')
    plt.legend(loc='lower right')
    
    # ì„±ëŠ¥ ë¹„êµ ê¸°ì¤€ì„  ì¶”ê°€ (ìˆ˜ë ´ ì†ë„ ë° ì•ˆì •ì„± ë¹„êµ ê¸°ì¤€)
    plt.axhline(y=200, color='r', linestyle='-', linewidth=1, label='Success Threshold (200)')
    
    plot_filename = 'learning_curve_comparison.png'
    plt.savefig(plot_filename)
    print(f"âœ… Learning curve saved to {plot_filename}. Please check the output directory.")
    try:
        plt.show() 
    except Exception:
        pass 

# ğŸ“Œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•¨ìˆ˜ (find_optimal_hyperparameters) ì œê±°

def main():
    """Run experiments for all three algorithms"""
    algorithms = ["DQN", "Double_DQN", "Dueling_DQN"]

    print("Choose algorithm to run:")
    print("1. DQN")
    print("2. Double DQN")
    print("3. Dueling DQN")
    print("4. Run all algorithms")
    # ğŸ“Œ 5. Hyperparameter Tuning ì˜µì…˜ ì œê±°

    choice = input("Enter your choice (1-4): ") # ğŸ“Œ ì‚¬ìš©ì ì…ë ¥ ë²”ìœ„ ë³€ê²½

    # Ask about rendering
    render_choice = input("Enable GUI visualization during training? (y/n): ").lower()
    render = render_choice in ['y', 'yes']

    # ğŸ“Œ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì—¬ë¶€ ë¬»ê¸°
    load_choice = input("Load previously saved model to continue training? (y/n): ").lower()
    load_model = load_choice in ['y', 'yes']

    q_net_to_evaluate = None
    all_history = [] 

    if choice == "1":
        q_net_to_evaluate, _, _ = run_experiment("DQN", render, load_model) 
    elif choice == "2":
        q_net_to_evaluate, _, _ = run_experiment("Double_DQN", render, load_model) 
    elif choice == "3":
        q_net_to_evaluate, _, _ = run_experiment("Dueling_DQN", render, load_model) 
    elif choice == "4":
        # ğŸ“Œ íŠœë‹ ë¡œì§ ì œê±° ë° ê³ ì • íŒŒë¼ë¯¸í„°ë¡œ ìˆœì°¨ ì‹¤í–‰
        print("\n--- Running All Algorithms with Fixed Parameters ---")
        
        for alg in algorithms:
            q_net, _, history = run_experiment(alg, render, load_model)
            all_history.append(history)
            evaluate_model(q_net, render=False) 
            
        plot_results(all_history, algorithms) 
        return
    # ğŸ“Œ 5ë²ˆ íŠœë‹ ì˜µì…˜ ê´€ë ¨ ë¡œì§ ì œê±°
    else:
        print("Invalid choice, running DQN by default")
        q_net_to_evaluate, _ , _ = run_experiment("DQN", render, load_model)

    # ğŸ“Œ ë‹¨ì¼ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ì‹œ, í•™ìŠµì´ ëë‚œ í›„ í‰ê°€ ì‹¤í–‰
    if q_net_to_evaluate is not None:
        evaluate_model(q_net_to_evaluate, render=False) 

if __name__ == '__main__':
    main()