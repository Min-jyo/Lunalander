# libraries
import gymnasium as gym
import collections
import random
import numpy as np

# pytorch library is used for deep learning
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# ğŸ’¡ íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë¦¬ë¥¼ ìœ„í•´ os ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import os 
# ğŸ’¡ ê·¸ë˜í”„ ì¶œë ¥ì„ ìœ„í•´ matplotlib ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import matplotlib.pyplot as plt
import pandas as pd

# --- Global Parameters (Used as defaults/overridden) ---
# NOTE: These are defaults, actual values are set in run_test from TEST_CONFIGS
learning_rate = 0.001
gamma = 0.98
buffer_limit = 50000        
batch_size = 32
tau = 1e-3                  
epsilon = 1.0               
PRINT_INTERVAL = 20         
TRAINING_EPISODES = 1000  # Full training episodes for graph resolution
EVAL_EPISODES = 20        # Evaluation episodes for final table metrics


# --- Classes and Core Functions ---

class ReplayBuffer():
    def __init__(self):
        self.buffer = collections.deque(maxlen=buffer_limit)    
    def put(self, transition):
        self.buffer.append(transition)
    def sample(self, n):
        mini_batch = random.sample(self.buffer, n)
        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []
        for transition in mini_batch:
            s, a, r, s_prime, done_mask = transition
            s_lst.append(s)
            a_lst.append([a])
            r_lst.append([r])
            s_prime_lst.append(s_prime)
            done_mask_lst.append([done_mask])
        return torch.tensor(np.array(s_lst), dtype=torch.float), torch.tensor(a_lst), \
               torch.tensor(r_lst, dtype=torch.float), torch.tensor(np.array(s_prime_lst), dtype=torch.float), \
               torch.tensor(done_mask_lst, dtype=torch.float)
    def size(self):
        return len(self.buffer)

class Qnet(nn.Module):
    def __init__(self):
        super(Qnet, self).__init__()
        self.fc1 = nn.Linear(8, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 4)
    def forward(self, x):
        if x.dim() == 1: x = x.unsqueeze(0)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
    def sample_action(self, obs, epsilon):
        out = self.forward(obs)
        coin = random.random()
        if coin < epsilon: return random.randint(0,3)
        else: return out.argmax(dim=1).item() 

class DuelingQnet(nn.Module):
    def __init__(self):
        super(DuelingQnet, self).__init__()
        self.fc1 = nn.Linear(8, 128)
        self.fc_value = nn.Linear(128, 128)
        self.fc_adv = nn.Linear(128, 128)
        self.value = nn.Linear(128, 1)
        self.adv = nn.Linear(128, 4)
    def forward(self, x):
        if x.dim() == 1: x = x.unsqueeze(0)
        x = F.relu(self.fc1(x))
        v = F.relu(self.fc_value(x))
        a = F.relu(self.fc_adv(x))
        v = self.value(v)
        a = self.adv(a)
        a_avg = torch.mean(a, dim=1, keepdim=True)
        q = v + a - a_avg
        return q
    def sample_action(self, obs, epsilon):
        out = self.forward(obs)
        coin = random.random()
        if coin < epsilon: return random.randint(0,3)
        else: return out.argmax(dim=1).item()

# ğŸ“Œ train_dqn/train_double_dqn í•¨ìˆ˜ê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¸ìˆ˜ë¡œ ë°›ì§€ ì•Šê³ , 
# ì „ì—­ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, run_testì—ì„œ ì „ì—­ ë³€ìˆ˜ë¥¼ ë¨¼ì € ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
def train_dqn(q, q_target, memory, optimizer):
    # Uses global batch_size, gamma, tau
    s,a,r,s_prime,done_mask = memory.sample(batch_size)
    q_out = q(s)
    q_a = q_out.gather(1,a)
    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)
    target = r + gamma * max_q_prime * done_mask
    loss = F.mse_loss(q_a, target)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    for target_param, local_param in zip(q_target.parameters(), q.parameters()):
        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)

def train_double_dqn(q, q_target, memory, optimizer):
    # Uses global batch_size, gamma, tau
    s,a,r,s_prime,done_mask = memory.sample(batch_size)
    q_out = q(s)
    q_a = q_out.gather(1,a)
    argmax_Q = q(s_prime).max(1)[1].unsqueeze(1)
    max_q_prime = q_target(s_prime).gather(1, argmax_Q)
    target = r + gamma * max_q_prime * done_mask
    loss = F.mse_loss(q_a, target)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    for target_param, local_param in zip(q_target.parameters(), q.parameters()):
        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)

def evaluate_model(q_net, num_episodes):
    env = gym.make('LunarLander-v3')
    total_score, success_count, total_length = 0.0, 0, 0.0
    for _ in range(num_episodes):
        s, _ = env.reset()
        done, episode_score, episode_length = False, 0.0, 0 
        with torch.no_grad():
            while not done:
                a = q_net.sample_action(torch.from_numpy(s).float(), 0.0)
                s_prime, r, terminated, truncated, info = env.step(a)
                done = (terminated or truncated)
                s = s_prime; episode_score += r; episode_length += 1
                if done: break
        total_score += episode_score
        if episode_score >= 200: success_count += 1
        total_length += episode_length
    env.close()
    return total_score / num_episodes, (success_count / num_episodes) * 100, total_length / num_episodes

# ğŸ’¡ run_test í•¨ìˆ˜ ìˆ˜ì •: LR, Gammaë¥¼ ì¸ìˆ˜ë¡œ ë°›ì•„ optimizer ìƒì„±ì— ì‚¬ìš©
def run_test(alg_type, train_fn, n_episodes, lr_val):
    global epsilon
    env = gym.make('LunarLander-v3')
    
    # Network Selection
    if alg_type == "Dueling_DQN":
        q, q_target = DuelingQnet(), DuelingQnet()
    else:
        q, q_target = Qnet(), Qnet()

    q_target.load_state_dict(q.state_dict())
    epsilon = 1.0 
    memory = ReplayBuffer()
    
    # ğŸ“Œ í˜„ì¬ í…ŒìŠ¤íŠ¸ì˜ LR ê°’ìœ¼ë¡œ Optimizer ìƒì„±
    optimizer = optim.Adam(q.parameters(), lr=lr_val) 
    
    score, score_history = 0.0, []
    
    # Gamma, batch_sizeëŠ” ì „ì—­ì—ì„œ ì„¤ì •ëœ í˜„ì¬ í…ŒìŠ¤íŠ¸ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    print(f"  [Training Start] LR: {lr_val}, Gamma: {gamma}, Batch: {batch_size}, Decay: 0.995")
    
    for n_epi in range(n_episodes): 
        s, _ = env.reset()
        done = False
        while not done:
            a = q.sample_action(torch.from_numpy(s).float(), epsilon)
            s_prime, r, terminated, truncated, info = env.step(a)
            done = (terminated or truncated)
            memory.put((s,a,r,s_prime, 0.0 if done else 1.0))
            s = s_prime
            score += r
            if memory.size()>2000:
                # train_fn í˜¸ì¶œ. ë‚´ë¶€ì ìœ¼ë¡œ global gamma, batch_size ì‚¬ìš©
                train_fn(q, q_target, memory, optimizer) 
            if done: break
            
        epsilon = max(0.01, epsilon * 0.995)
        
        if n_epi % PRINT_INTERVAL == 0 and n_epi != 0: 
            avg_score = score / PRINT_INTERVAL
            print(f"    Epi: {n_epi:<4} / {n_episodes} | Avg Score: {avg_score:.2f} | Buffer: {memory.size():<5} | Epsilon: {epsilon*100:.1f}%")
            score_history.append((n_epi, avg_score))
            score = 0.0

    env.close()
    avg_return, success_rate, avg_length = evaluate_model(q, EVAL_EPISODES)
    print(f"  [Evaluation End] Avg Return: {avg_return:.2f}, Success Rate: {success_rate:.2f}%")
    return avg_return, success_rate, avg_length, score_history

# ğŸ’¡ Plotting í•¨ìˆ˜: ëª¨ë“  6ê°œ ë¼ì¸ì„ í•œ ê·¸ë˜í”„ì— ê·¸ë¦½ë‹ˆë‹¤.
def plot_results(all_history_data, all_history_labels):
    """í•™ìŠµ ê³¡ì„ (í‰ê·  ë¦¬í„´ê°’ vs ì—í”¼ì†Œë“œ)ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    
    # Define color/style pairs for clear separation of V1 vs V2
    styles = [
        ('darkviolet', 'DQN V1', '--'), ('darkgreen', 'Dueling V1', '-'), ('red', 'Double V1', '-.'),
        ('dodgerblue', 'DQN V2', '--'), ('lime', 'Dueling V2', '-'), ('orange', 'Double V2', '-.'),
    ]
    
    plt.figure(figsize=(10, 6))
    
    for history, label in zip(all_history_data, all_history_labels):
        # ğŸ“Œ ìˆ˜ì •ëœ ë¶€ë¶„: s ëŒ€ì‹  (color, l, ls) íŠœí”Œ ì „ì²´ë¥¼ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
        style_tuple = next((color, l, ls) for color, l, ls in styles if l == label or (label.startswith(l.split()[0]) and label.endswith(l.split()[-1])))
        color, linestyle = style_tuple[0], style_tuple[2]

        episodes = [item[0] for item in history]
        scores = [item[1] for item in history]
        
        plt.plot(episodes, scores, label=label, color=color, linestyle=linestyle, linewidth=2 if linestyle == '-' else 1)

    plt.title('DQN Algorithms Performance Comparison (Average Return per Episode)')
    plt.xlabel(f'Episode (Average of {PRINT_INTERVAL} episodes)') 
    plt.ylabel('Average Return (Score)')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend(loc='lower right', ncol=2, fontsize='small') # ë²”ë¡€ë¥¼ 2ì—´ë¡œ í‘œì‹œ
    
    plt.axhline(y=200, color='r', linestyle='--', linewidth=1, label='Success Threshold (200)')
    
    plot_filename = 'learning_curve_comparison.png'
    plt.savefig(plot_filename)
    print(f"\nâœ… Learning curve saved to {plot_filename}.")
    try:
        plt.show() 
    except Exception:
        pass 


# --- Main Execution Loop ---
TEST_CONFIGS = [
    # V1 Configuration
    {'version': 'V1', 'alg': 'DQN', 'fn': train_dqn, 'params': {'lr': 0.005, 'gamma': 0.98, 'batch_size': 32}},
    {'version': 'V1', 'alg': 'Dueling DQN', 'fn': train_double_dqn, 'params': {'lr': 0.005, 'gamma': 0.98, 'batch_size': 32}}, 
    {'version': 'V1', 'alg': 'Double DQN', 'fn': train_double_dqn, 'params': {'lr': 0.005, 'gamma': 0.98, 'batch_size': 32}},
    
    # V2 Configuration
    {'version': 'V2', 'alg': 'DQN', 'fn': train_dqn, 'params': {'lr': 0.001, 'gamma': 0.99, 'batch_size': 64}},
    {'version': 'V2', 'alg': 'Dueling DQN', 'fn': train_double_dqn, 'params': {'lr': 0.001, 'gamma': 0.99, 'batch_size': 64}},
    {'version': 'V2', 'alg': 'Double DQN', 'fn': train_double_dqn, 'params': {'lr': 0.001, 'gamma': 0.99, 'batch_size': 64}},
]

results = []
all_history_data = []
all_history_labels = []

print(f"==============================================================")
print(f"ğŸ”¥ Starting 6 Total Experiments (Training: {TRAINING_EPISODES}, Evaluation: {EVAL_EPISODES})")
print(f"==============================================================")

for i, config in enumerate(TEST_CONFIGS):
    # ğŸ“Œ global ì„ ì–¸ ì œê±°: ìŠ¤í¬ë¦½íŠ¸ ë ˆë²¨ì—ì„œ ë°”ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    learning_rate = config['params']['lr']
    gamma = config['params']['gamma']
    batch_size = config['params']['batch_size']
    
    alg_name = f"{config['alg']} {config['version']}"
    
    print(f"\n--- Running Test {i+1}/6: {alg_name} ---")
    
    # Run training and collect history (LR ê°’ë§Œ run_testì— ì¸ìˆ˜ë¡œ ì „ë‹¬)
    avg_return, success_rate, avg_length, score_history = run_test(
        config['alg'].replace(' ', '_'), config['fn'], TRAINING_EPISODES, learning_rate
    )
    
    # Collect results for table
    results.append({
        'Algorithm': alg_name,
        'í‰ê· ë¦¬í„´': f"{avg_return:.2f}",
        'ì„±ê³µë¥ ': f"{success_rate:.2f}%",
        'ì—í”¼ì†Œë“œê¸¸ì´': f"{avg_length:.1f}",
        'ìˆ˜ë ´ì†ë„': 'ë¼ì¸ ê·¸ë˜í”„ ì°¸ì¡°',
        'í•™ìŠµì•ˆì •ì„±': 'ë¼ì¸ ê·¸ë˜í”„ ì°¸ì¡°',
        'ë°ì´í„°íš¨ìœ¨ì„±': 'ê³ ì •'
    })
    
    # Collect history for graph
    all_history_data.append(score_history)
    all_history_labels.append(alg_name)

# --- Final Result Output (Table & Graph) ---
print(f"\n==============================================================")
print(f"âœ… All Experiments Completed (Evaluation Episodes: {EVAL_EPISODES} each)")
print(f"==============================================================")

df = pd.DataFrame(results)
print(df.to_string(index=False))

# Generate and display the final comparison graph
plot_results(all_history_data, all_history_labels)