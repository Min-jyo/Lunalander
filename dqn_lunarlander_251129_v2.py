# libraries
import gymnasium as gym
import collections
import random
import numpy as np

# pytorch library is used for deep learning
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# ğŸ’¡ íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë¦¬ë¥¼ ìœ„í•´ os ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import os 
# ğŸ’¡ ê·¸ë˜í”„ ì¶œë ¥ì„ ìœ„í•´ matplotlib ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import matplotlib.pyplot as plt

# hyperparameters
learning_rate = 0.001
gamma = 0.98
buffer_limit = 50000        # size of replay buffer
batch_size = 32
tau = 1e-3                  # for soft update
epsilon = 1.0               # Epsilonì„ ì „ì—­ ë³€ìˆ˜ë¡œ ê´€ë¦¬
PRINT_INTERVAL = 20         # ğŸ’¡ í‰ê·  ì ìˆ˜ ê³„ì‚° ë° ì¶œë ¥ ê°„ê²© (Globalë¡œ ì •ì˜)

class ReplayBuffer():
    def __init__(self):
        self.buffer = collections.deque(maxlen=buffer_limit)    # double-ended queue

    def put(self, transition):
        self.buffer.append(transition)

    def sample(self, n):
        mini_batch = random.sample(self.buffer, n)
        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []

        for transition in mini_batch:
            s, a, r, s_prime, done_mask = transition
            s_lst.append(s)
            a_lst.append([a])
            r_lst.append([r])
            s_prime_lst.append(s_prime)
            done_mask_lst.append([done_mask])

        return torch.tensor(np.array(s_lst), dtype=torch.float), torch.tensor(a_lst), \
               torch.tensor(r_lst, dtype=torch.float), torch.tensor(np.array(s_prime_lst), dtype=torch.float), \
               torch.tensor(done_mask_lst, dtype=torch.float)

    def size(self):
        return len(self.buffer)

class Qnet(nn.Module):
    def __init__(self):
        super(Qnet, self).__init__()
        self.fc1 = nn.Linear(8, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 4)

    def forward(self, x):
        # ğŸ› ì˜¤ë¥˜ í•´ê²°: ë‹¨ì¼ ìƒ˜í”Œì¼ ê²½ìš° ë°°ì¹˜ ì°¨ì›(dim=0)ì´ ì—†ìœ¼ë¯€ë¡œ,
        # ë°°ì¹˜ ì…ë ¥ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ unsqueeze(0)ë¥¼ ì ìš©í•˜ë„ë¡ ì²˜ë¦¬
        if x.dim() == 1:
            x = x.unsqueeze(0)
            
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def sample_action(self, obs, epsilon):
        # ğŸ› ì˜¤ë¥˜ í•´ê²°: forward í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ unsqueezeë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” obsë§Œ ì „ë‹¬
        out = self.forward(obs)
        coin = random.random()
        if coin < epsilon:
            return random.randint(0,3)
        else:
            # ë‹¨ì¼ ì…ë ¥ ì‹œ outì€ [1, 4] í¬ê¸°ì´ë¯€ë¡œ dim=1ì—ì„œ argmaxë¥¼ ì·¨í•©ë‹ˆë‹¤.
            return out.argmax(dim=1).item() 

class DuelingQnet(nn.Module):
    def __init__(self):
        super(DuelingQnet, self).__init__()
        self.fc1 = nn.Linear(8, 128)
        self.fc_value = nn.Linear(128, 128)
        self.fc_adv = nn.Linear(128, 128)
        self.value = nn.Linear(128, 1)
        self.adv = nn.Linear(128, 4)

    def forward(self, x):
        # ğŸ› ì˜¤ë¥˜ í•´ê²°: ë‹¨ì¼ ìƒ˜í”Œì¼ ê²½ìš° ë°°ì¹˜ ì°¨ì›(dim=0)ì´ ì—†ìœ¼ë¯€ë¡œ,
        # ë°°ì¹˜ ì…ë ¥ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ unsqueeze(0)ë¥¼ ì ìš©í•˜ë„ë¡ ì²˜ë¦¬
        if x.dim() == 1:
            x = x.unsqueeze(0)
            
        x = F.relu(self.fc1(x))
        v = F.relu(self.fc_value(x))
        a = F.relu(self.fc_adv(x))
        v = self.value(v)
        a = self.adv(a)
        
        # ğŸ› ì˜¤ë¥˜ í•´ê²°: ì´ì œ xê°€ ìµœì†Œ [1, 8] í˜•íƒœì´ë¯€ë¡œ aëŠ” [ë°°ì¹˜í¬ê¸°, 4] í˜•íƒœë¥¼ ê°€ì§
        # ë”°ë¼ì„œ dim=1ì—ì„œ í‰ê· ì„ ì·¨í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.
        a_avg = torch.mean(a, dim=1, keepdim=True)
        q = v + a - a_avg
        return q

    def sample_action(self, obs, epsilon):
        # ğŸ› ì˜¤ë¥˜ í•´ê²°: forward í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ unsqueezeë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” obsë§Œ ì „ë‹¬
        out = self.forward(obs)
        coin = random.random()
        if coin < epsilon:
            return random.randint(0,3)
        else:
            # ë‹¨ì¼ ì…ë ¥ ì‹œ outì€ [1, 4] í¬ê¸°ì´ë¯€ë¡œ dim=1ì—ì„œ argmaxë¥¼ ì·¨í•©ë‹ˆë‹¤.
            return out.argmax(dim=1).item()

def train_dqn(q, q_target, memory, optimizer, tau):
    """Standard DQN training"""
    s,a,r,s_prime,done_mask = memory.sample(batch_size)

    q_out = q(s)
    q_a = q_out.gather(1,a)

    # DQN
    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)

    target = r + gamma * max_q_prime * done_mask
    loss = F.mse_loss(q_a, target)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Soft update
    for target_param, local_param in zip(q_target.parameters(), q.parameters()):
        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)

def train_double_dqn(q, q_target, memory, optimizer, tau):
    """Double DQN training"""
    s,a,r,s_prime,done_mask = memory.sample(batch_size)

    q_out = q(s)
    q_a = q_out.gather(1,a)

    # Double DQN
    argmax_Q = q(s_prime).max(1)[1].unsqueeze(1)
    max_q_prime = q_target(s_prime).gather(1, argmax_Q)

    target = r + gamma * max_q_prime * done_mask
    loss = F.mse_loss(q_a, target)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Soft update
    for target_param, local_param in zip(q_target.parameters(), q.parameters()):
        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)

# ğŸ’¡ run_experiment í•¨ìˆ˜ì— lrê³¼ decay_rate ì¸ì ì¶”ê°€
def run_experiment(algorithm_type="DQN", render=False, load_model=False, lr=learning_rate, decay_rate=0.995):
    """ì§€ì •ëœ ì•Œê³ ë¦¬ì¦˜ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì‹¤í—˜ì„ ì‹¤í–‰í•˜ê³  í•™ìŠµëœ ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global epsilon
    
    # íŠœë‹ ì¤‘ì—ëŠ” í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì˜¤ì§€ ì•ŠìŒ (ë‹¨ì¼ ì‹¤í—˜ ëª¨ë“œì—ì„œë§Œ íŒŒì¼ ì €ì¥/ë¡œë“œ)
    is_tuning = (lr != learning_rate or decay_rate != 0.995) 

    if is_tuning:
        # íŠœë‹ ëª¨ë“œì—ì„œ ì‹¤í–‰í•  ê²½ìš°, ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì„ íŠœë‹í•˜ëŠ”ì§€ ëª…ì‹œ (ì˜µì…˜ 4ì˜ ì²« ë‹¨ê³„)
        print(f"  [Tuning] {algorithm_type} - LR: {lr:.1e}, Decay: {decay_rate:.4f}...")
    else:
        print(f"\n=== Running {algorithm_type} Experiment ===")

    if render:
        env = gym.make('LunarLander-v3', render_mode='human')
        # íŠœë‹ ëª¨ë“œì—ì„œëŠ” ë Œë”ë§ ë©”ì‹œì§€ ìƒëµ
        if not is_tuning:
            print("Rendering enabled - GUI window will show LunarLander visualization")
    else:
        env = gym.make('LunarLander-v3')

    # ë„¤íŠ¸ì›Œí¬ ì„ íƒ
    if algorithm_type == "Dueling_DQN":
        q = DuelingQnet()
        q_target = DuelingQnet()
        train_fn = train_double_dqn
    else:
        q = Qnet()
        q_target = Qnet()
        train_fn = train_dqn if algorithm_type == "DQN" else train_double_dqn

    model_path = f'./{algorithm_type}_q_net.pth'
    
    # ğŸ“Œ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (Load Model): íŠœë‹ ëª¨ë“œì—ì„œëŠ” ë¶ˆëŸ¬ì˜¤ì§€ ì•ŠìŒ
    if load_model and os.path.exists(model_path) and not is_tuning:
        print(f"Loading previous model from {model_path} to continue training.")
        try:
            q.load_state_dict(torch.load(model_path))
            q_target.load_state_dict(q.state_dict())
            epsilon = 0.1 
        except Exception as e:
            print(f"Error loading model state: {e}. Starting new training.")
            q_target.load_state_dict(q.state_dict())
            epsilon = 1.0
    else:
        q_target.load_state_dict(q.state_dict())
        epsilon = 1.0 # ìƒˆë¡œ í•™ìŠµ ì‹œì‘ ì‹œ epsilon ì´ˆê¸°í™”

    memory = ReplayBuffer()

    score = 0.0
    # ğŸ’¡ ì „ë‹¬ë°›ì€ lrë¡œ Optimizer ì´ˆê¸°í™”
    optimizer = optim.Adam(q.parameters(), lr=lr) 
    
    # ğŸ’¡ í•™ìŠµ ê³¡ì„  ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
    score_history = []
    
    # ğŸ’¡ ì—í”¼ì†Œë“œ íšŸìˆ˜ëŠ” 1000ìœ¼ë¡œ ìœ ì§€
    for n_epi in range(1000): 
        s, _ = env.reset()
        done = False

        while not done:
            a = q.sample_action(torch.from_numpy(s).float(), epsilon)
            s_prime, r, terminated, truncated, info = env.step(a)
            done = (terminated or truncated)
            done_mask = 0.0 if done else 1.0
            memory.put((s,a,r,s_prime, done_mask))
            s = s_prime

            score += r

            if memory.size()>2000:
                train_fn(q, q_target, memory, optimizer, tau)

            if done:
                break
        
        # ğŸ’¡ ì „ë‹¬ë°›ì€ decay_rateë¡œ Epsilon decay ì ìš©
        epsilon = max(0.01, epsilon * decay_rate) 

        if n_epi % PRINT_INTERVAL == 0 and n_epi != 0: 
            avg_score = score / PRINT_INTERVAL
            if not is_tuning: # íŠœë‹ ì¤‘ì—ëŠ” ì¶œë ¥ ìƒëµ
                print("n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%".format(
                                                            n_epi, avg_score, memory.size(), epsilon*100))
            score_history.append((n_epi, avg_score))
            score = 0.0

    env.close()
    
    # ğŸ“Œ ëª¨ë¸ ì €ì¥ (Save Model): íŠœë‹ ëª¨ë“œì—ì„œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
    if not is_tuning:
        torch.save(q.state_dict(), model_path)
        print(f"\nModel for {algorithm_type} saved to {model_path}")
    
    return q, q_target, score_history 

def evaluate_model(q_net, env_name='LunarLander-v3', num_episodes=100, render=False):
    """
    ìµœì¢… í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥(í‰ê·  ë¦¬í„´ê°’, ì„±ê³µë¥ , í‰ê·  ê¸¸ì´)ì„ í‰ê°€í•©ë‹ˆë‹¤.
    ì„±ê³µ ê¸°ì¤€ì€ ìŠ¤ì½”ì–´ 200ì  ì´ìƒì…ë‹ˆë‹¤.
    """
    # íŠœë‹ ëª¨ë“œì—ì„œëŠ” í‰ê°€ ê²°ê³¼ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
    print_summary = True if num_episodes == 100 else False
    
    if print_summary:
        print(f"\n=== Evaluating Model over {num_episodes} episodes ===")
    
    if render:
        env = gym.make(env_name, render_mode='human')
    else:
        env = gym.make(env_name)
    
    total_score = 0.0
    success_count = 0  # ì„±ê³µ íšŸìˆ˜ ì¹´ìš´í„°
    total_length = 0.0 # ì´ ì—í”¼ì†Œë“œ ê¸¸ì´ ëˆ„ì  ë³€ìˆ˜
    
    epsilon_eval = 0.0 # í‰ê°€ ì‹œ íƒí—˜ ì—†ìŒ (ìµœì  í–‰ë™ë§Œ)

    for n_epi in range(num_episodes):
        s, _ = env.reset()
        done = False
        episode_score = 0.0
        episode_length = 0 

        with torch.no_grad(): # í‰ê°€ ì‹œì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¶ˆí•„ìš”
            while not done:
                a = q_net.sample_action(torch.from_numpy(s).float(), epsilon_eval)
                s_prime, r, terminated, truncated, info = env.step(a)
                done = (terminated or truncated)
                s = s_prime
                episode_score += r
                episode_length += 1
                
                if done:
                    break
        
        total_score += episode_score
        
        if episode_score >= 200:
            success_count += 1
            
        total_length += episode_length

    env.close()
    
    avg_return = total_score / num_episodes
    success_rate = (success_count / num_episodes) * 100
    avg_length = total_length / num_episodes

    if print_summary:
        print(f"\n[Evaluation Results - {num_episodes} Episodes]")
        print(f"âœ… Average Return: {avg_return:.2f}")
        print(f"ğŸ‰ Success Rate (Score >= 200): {success_rate:.2f}%")
        print(f"â±ï¸ Average Episode Length: {avg_length:.1f} steps")
    
    return avg_return, success_rate, avg_length

def plot_results(all_history, algorithms):
    """í•™ìŠµ ê³¡ì„ (í‰ê·  ë¦¬í„´ê°’ vs ì—í”¼ì†Œë“œ)ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    print("\n=== Generating Learning Curve Plot ===")
    
    plt.figure(figsize=(10, 6))
    
    for history, alg_name in zip(all_history, algorithms):
        episodes = [item[0] for item in history]
        scores = [item[1] for item in history]
        
        # ê° ì•Œê³ ë¦¬ì¦˜ë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ ë¼ì¸ ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
        plt.plot(episodes, scores, label=alg_name)

    plt.title('DQN Algorithms Performance Comparison (Average Return)')
    plt.xlabel(f'Episode (Average of {PRINT_INTERVAL} episodes)') 
    plt.ylabel('Average Return (Score)')
    plt.grid(True, linestyle='--')
    plt.legend(loc='lower right')
    
    # ì„±ëŠ¥ ë¹„êµ ê¸°ì¤€ì„  ì¶”ê°€ (ìˆ˜ë ´ ì†ë„ ë° ì•ˆì •ì„± ë¹„êµ ê¸°ì¤€)
    plt.axhline(y=200, color='r', linestyle='-', linewidth=1, label='Success Threshold (200)')
    
    plot_filename = 'learning_curve_comparison.png'
    plt.savefig(plot_filename)
    print(f"âœ… Learning curve saved to {plot_filename}. Please check the output directory.")
    try:
        plt.show() 
    except Exception:
        pass 

# ğŸ’¡ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•¨ìˆ˜ ì´ë¦„ ë³€ê²½ ë° ìµœì  íŒŒë¼ë¯¸í„° ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
def find_optimal_hyperparameters(algorithm_type="Dueling_DQN"):
    """
    ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°(Learning Rate, Epsilon Decay Rate)ë¥¼ ê·¸ë¦¬ë“œ íƒìƒ‰í•˜ê³  ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"\n=======================================================")
    print(f"ğŸ¯ Starting Hyperparameter Tuning for {algorithm_type}")
    print(f"=======================================================")

    # ğŸ’¡ íƒìƒ‰ ë²”ìœ„ í™•ì¥: Learning RateëŠ” 1e-5ë¶€í„° 1e-2ê¹Œì§€ 7ê°œ
    learning_rates = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]
    # ğŸ’¡ íƒìƒ‰ ë²”ìœ„ í™•ì¥: Decay RateëŠ” 0.990ë¶€í„° 0.999ê¹Œì§€ 5ê°œ
    decay_rates = [0.990, 0.993, 0.995, 0.997, 0.999]

    best_score = -np.inf
    best_params = None
    
    results = []
    
    total_combinations = len(learning_rates) * len(decay_rates)
    print(f"Total combinations to test: {total_combinations} combinations.")

    # ê·¸ë¦¬ë“œ íƒìƒ‰ ì‹œì‘
    for lr in learning_rates:
        for decay in decay_rates:
            # ëª¨ë¸ í•™ìŠµ (run_experiment): íŠœë‹ ëª¨ë“œëŠ” is_tuning=Trueë¡œ ìë™ ì„¤ì •ë˜ì–´ ì¶œë ¥ ìƒëµ
            q_net, _, _ = run_experiment(algorithm_type, False, False, lr=lr, decay_rate=decay)
            
            # ëª¨ë¸ í‰ê°€ (evaluate_model)
            avg_return, success_rate, avg_length = evaluate_model(q_net, num_episodes=50)

            results.append({
                'lr': lr,
                'decay': decay,
                'avg_return': avg_return,
                'success_rate': success_rate
            })

            if avg_return > best_score:
                best_score = avg_return
                best_params = {'lr': lr, 'decay': decay}

    print("\n=======================================================")
    print("ğŸ“ˆ Tuning Results Summary:")
    print(f"{'LR':<8} {'Decay Rate':<12} {'Avg Return':<12} {'Success Rate'}")
    print("-" * 45)
    for res in results:
        print(f"{res['lr']:.1e:<8} {res['decay']:.4f:<12} {res['avg_return']:.2f:<12} {res['success_rate']:.2f}%")
        
    print("\nğŸ¥‡ Best Hyperparameters Found:")
    print(f"   Learning Rate: {best_params['lr']:.1e}")
    print(f"   Decay Rate: {best_params['decay']:.4f}")
    print(f"   Best Avg Return: {best_score:.2f}")
    print("=======================================================")
    
    # ğŸ’¡ ìµœì  íŒŒë¼ë¯¸í„° ë°˜í™˜
    return best_params


def main():
    """Run experiments for all three algorithms"""
    algorithms = ["DQN", "Double_DQN", "Dueling_DQN"]

    print("Choose algorithm to run:")
    print("1. DQN")
    print("2. Double DQN")
    print("3. Dueling DQN")
    print("4. Run all algorithms (Comparison Plot)")
    print("5. Hyperparameter Tuning (Find Optimal LR/Decay)") # ğŸ’¡ íŠœë‹ ì˜µì…˜ ì¶”ê°€

    choice = input("Enter your choice (1-5): ")

    # Ask about rendering
    render_choice = input("Enable GUI visualization during training? (y/n): ").lower()
    render = render_choice in ['y', 'yes']

    # ğŸ“Œ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì—¬ë¶€ ë¬»ê¸°
    load_choice = input("Load previously saved model to continue training? (y/n): ").lower()
    load_model = load_choice in ['y', 'yes']

    q_net_to_evaluate = None
    all_history = [] 

    if choice == "1":
        q_net_to_evaluate, _, _ = run_experiment("DQN", render, load_model) 
    elif choice == "2":
        q_net_to_evaluate, _, _ = run_experiment("Double_DQN", render, load_model) 
    elif choice == "3":
        q_net_to_evaluate, _, _ = run_experiment("Dueling_DQN", render, load_model) 
    elif choice == "4":
        # 1. Optimal parameter tuning (Dueling_DQNì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°)
        print("\n--- Step 1: Finding Optimal Hyperparameters for Comparison (using Dueling_DQN) ---")
        best_params = find_optimal_hyperparameters("Dueling_DQN") 
        
        optimal_lr = best_params['lr']
        optimal_decay = best_params['decay']
        
        print(f"\n--- Optimal Parameters Selected for Comparison: LR={optimal_lr:.1e}, Decay={optimal_decay:.4f} ---")
        print("--- Step 2: Running All Algorithms with Optimal Parameters ---")
        
        for alg in algorithms:
            # 2. Run experiment with optimal parameters
            # íŠœë‹ëœ LRê³¼ Decay Rateë¥¼ run_experimentì— ì „ë‹¬
            q_net, _, history = run_experiment(alg, render, load_model, lr=optimal_lr, decay_rate=optimal_decay)
            all_history.append(history)
            evaluate_model(q_net, render=False) 
            
        # 3. Plot results
        plot_results(all_history, algorithms) 
        return
    elif choice == "5": # ğŸ’¡ íŠœë‹ ëª¨ë“œ ì‹¤í–‰
        # Dueling DQNì— ëŒ€í•œ íŠœë‹ ì‹¤í–‰ 
        find_optimal_hyperparameters("Dueling_DQN")
        return
    else:
        print("Invalid choice, running DQN by default")
        q_net_to_evaluate, _ , _ = run_experiment("DQN", render, load_model)

    # ğŸ“Œ ë‹¨ì¼ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ ì‹œ, í•™ìŠµì´ ëë‚œ í›„ í‰ê°€ ì‹¤í–‰
    if q_net_to_evaluate is not None:
        evaluate_model(q_net_to_evaluate, render=False) 

if __name__ == '__main__':
    main()